# Large-Scale Training

# 12 - Deep Learning Models

Just like in the previous video, we're going to put the practical in the academic 
and take a deeper—pun, as usual, intended—look at deep-learning models.

Fire up that play button!

# Key Terms

# [Model_Parallelism]
A machine learning model training strategy used to maximize the utilization of 
compute resources (CPUs/GPUs) in which the model is distributed across two or 
more devices.
    
# [Data_Parallelism]
A machine learning model training strategy used to maximize the utilization of 
compute resources (CPUs/GPUs) in which the data is distributed across two or 
more devices
    
# [Graphics_Processing_Unit]

  A specialized device that has many cores, allowing it to perform many
  operations at a time.

  GPUs are often used within deep learning to accelerate training of neural
  networks by taking advantage of their ability to perform many parallel
  computations.

# [Concurrency]
When two or computer programs share a single processor.
