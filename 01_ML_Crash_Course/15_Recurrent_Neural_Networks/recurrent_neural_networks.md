# Deep Learning

# 15 - Recurrent Neural Networks

Here
Here we
Here we cover
Here we cover RNNs [EOS]

# Key Terms

# [Recurrent_Neural_Network]
Also RNN, a neural network in which the output is routed back into the network to 
be used with the subsequent input.

# [Hidden_State]
The output of the Recurrent Neural Network at the previous timestep.
    
# [Cell_State]
Used within a Long short-term memory (LSTM) acting as an additional hidden state.
    
# [Backpropagation_Through_Time]
Backpropagation within an RNN in which an additional partial derivative term is 
calculated per time step that the input required.

# [Long_Short-term_Memory]
A type of RNN with a cell and hidden state, input gate, forget gate, and an output 
gate.
    
# [Gated_Recurrent_Unit]
A type of RNN with a hidden state, update gate, and reset gate.
    
# [Embedding_Layer]
Typically used as the first layer in a neural network which is optimized to 
transform the input in an effort to maximize.
    
# [Gradient_Clipping]
Capping the value that the gradient is allowed to be. This is typically used in an 
effort to avoid exploding gradients. However, initialization techniques are favored.
