# 4 - Performance
Is your model any good? Is it an improvement over your heuristic? How do you even 
measure and quantify this? 

In this video, we'll be going over techniques you can use to evaluate and tune 
your model's performance in order to ensure it boasts...model performance.

# Key Terms

# [Decision_Point]
Also decision rule or threshold, is a cut-off point in which anything below the 
cutoff is determined to be a certain class and anything above the cut-off is the 
other class.
    
# [Accuracy]
The number of true positives plus the number of true negatives divided by the 
total number of examples.

# [Unbalanced_Classes]
When one class is far more frequently observed than another class.
    
# [Model_Training]
Determining the model parameter values.

# [Confusion_Matrix]
In the binary case, a 2x2 matrix indicating the number of true positives, true 
negatives, false positives and false negatives.
    
# [Sensitivity]
Also recall, is the proportion of true positives which are correctly classified.

# [Specificity]
The proportion of true negatives which are correctly classified.

# [Precision]
The number of true positives divided by the true positives plus false positives.

# [F1_Score]
The harmonic mean of the precision and recall.

# [Validation]
The technique of holding out some portion of examples to be tested separately 
from the set of examples used to train the model.

# [Generalize]
The ability of a model to perform well on the test set as well as examples beyond 
the test set.
    
# [Receiver_Operator_Characteristic_Curve]
Also ROC curve, is a plot of how the specificity and sensitivity change as the 
decision threshold changes. The area under the ROC curve, or AUC, is the probability 
that a randomly chosen positive example will have a higher prediction probability of 
being positive than a randomly chosen negative example.
    
# [Hyperparameter]
Any parameter associated with a model which is not learned.
