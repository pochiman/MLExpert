# Supervised Learning

# 10 - Support Vector Machine

Why do support vectors always look sad?
Because they feel marginalized.

# Key Terms

# [Support_Vectors]
The most difficult to separate points in regard to a decision boundary. They influence 
the location and orientation of the hyperplane.

# [Margin]
The space between the hyperplane and the support vectors. In the case of soft margin 
Support Vector Machines, this margin includes slack.

# [Hyperplane] π
A decision boundary in any dimension.

# [Norm] π
Here, the L2 Norm, is the square root of the sum of squares of each element in a 
vector.

# [Outlier]
A feature or group of features which vary significantly from the other features.

# [Slack]
The relaxing of the constraint that all examples must lie outside of the margin. 
This creates a soft-margin SVM.

# [Hinge_Loss]
A loss function which is used by a soft-margin SVM.

# [Sub-gradient]
The gradient of a non-differentiable function.

# [Non-differentiable] π
A function which has kinks in which a derivative is not defined.

# [Convex_Function] π
Function with one optima.

# [Kernel_Trick]
The process of finding the dot product of a high dimensional representation of 
feature without computing the high dimensional representation itself. A common 
kernel is the Radial Basis Function kernel.
